# Enterprise Doc Bot

A self-hosted Retrieval-Augmented-Generation (RAG) assistant that ingests **GitHub repositories** and **Confluence docs** to answer questions about codebases and documentation using local LLMs.

## âœ¨ Features
- **Multi-source ingestion** â€“ GitHub repos, Confluence spaces, local docs
- **Vector + keyword hybrid search** for accurate retrieval
- **CPU-friendly runtime** â€“ runs locally using quantized models via Ollama
- **Streamlit web UI** â€“ chat interface with source citations
- **CLI interface** â€“ command-line tools for ingestion and chat
- **Privacy-first** â€“ everything runs locally, no external API calls

## ğŸš€ Quick start

### 1. Install Prerequisites
```bash
# Install Ollama (local LLM server)
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service (required before pulling models)
ollama serve &

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt
```

### 2. Setup LLM Model
```bash
# Pull a quantized model (this may take a few minutes)
ollama pull mistral:7b-instruct-q4_0

# Verify model is available
ollama list
```

### 3. Configure Sources
Edit `config.yaml` to add your GitHub repos and Confluence spaces:

```yaml
sources:
  git:
    - url: "https://github.com/your-org/your-repo.git"
      name: "Your Project"
      branch: "main"
  
  knowledge_base:
    - url: "https://your-company.atlassian.net/wiki"
      type: "confluence"
      name: "Company Docs"
    - path: "./docs"
      type: "local"
      name: "Local Documentation"
```

### 4. Ingest & Serve
```bash
# Ingest documents (this may take several minutes)
python main.py ingest

# Launch web interface at http://localhost:8501
python main.py serve
```

### 5. Alternative: CLI Chat
```bash
# Interactive command-line chat
python main.py chat --interactive

# One-time search
python main.py search "How do I configure the API?"

# Check system status
python main.py status
```

## ğŸ”§ Environment Variables
For private repositories and Confluence, set these:
```bash
export GITHUB_TOKEN="your_github_token"
export CONFLUENCE_USERNAME="your_email@company.com"
export CONFLUENCE_TOKEN="your_confluence_token"
```

## ğŸ“Š Commands Reference
- `python main.py ingest` - Process all configured sources
- `python main.py ingest --force` - Re-process everything (clears index)
- `python main.py serve` - Start web interface
- `python main.py chat -i` - Interactive CLI chat
- `python main.py search "query"` - Search knowledge base
- `python main.py status` - Show system statistics

## ğŸ› Troubleshooting
- **"Ollama not available"**: Run `ollama serve` first
- **"No documents found"**: Run `python main.py ingest` first
- **Import errors**: Activate virtual environment with `source venv/bin/activate`
- **Rate limits**: Set `GITHUB_TOKEN` for higher GitHub API limits
- **Memory issues**: Use smaller models like `mistral:7b-instruct-q4_0`

## ğŸ“ Project Structure
```
enterprise-doc-bot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ shared/          # Config, logging, text processing
â”‚   â”œâ”€â”€ ingest/          # GitHub & Confluence ingestion
â”‚   â”œâ”€â”€ search/          # Vector store & hybrid search
â”‚   â”œâ”€â”€ chat/            # LLM client & chat interface
â”‚   â””â”€â”€ web/             # Streamlit web UI
â”œâ”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ main.py             # CLI entry point
```